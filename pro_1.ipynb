{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>...</th>\n",
       "      <th>f_1190</th>\n",
       "      <th>f_1191</th>\n",
       "      <th>f_1192</th>\n",
       "      <th>f_1193</th>\n",
       "      <th>f_1194</th>\n",
       "      <th>f_1195</th>\n",
       "      <th>f_1196</th>\n",
       "      <th>f_1197</th>\n",
       "      <th>f_1198</th>\n",
       "      <th>f_1199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.033875</td>\n",
       "      <td>0.978446</td>\n",
       "      <td>-0.142131</td>\n",
       "      <td>-0.177117</td>\n",
       "      <td>-1.470684</td>\n",
       "      <td>1.669562</td>\n",
       "      <td>-0.196530</td>\n",
       "      <td>-0.125239</td>\n",
       "      <td>-0.452284</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.111266</td>\n",
       "      <td>0.716084</td>\n",
       "      <td>0.060039</td>\n",
       "      <td>0.301279</td>\n",
       "      <td>-1.174846</td>\n",
       "      <td>-1.076498</td>\n",
       "      <td>-0.069452</td>\n",
       "      <td>-0.604012</td>\n",
       "      <td>-2.179176</td>\n",
       "      <td>0.558003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.348835</td>\n",
       "      <td>0.294815</td>\n",
       "      <td>-0.557577</td>\n",
       "      <td>-2.020773</td>\n",
       "      <td>-1.234715</td>\n",
       "      <td>1.633930</td>\n",
       "      <td>-1.680658</td>\n",
       "      <td>-0.358146</td>\n",
       "      <td>0.166122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735240</td>\n",
       "      <td>0.829781</td>\n",
       "      <td>1.521941</td>\n",
       "      <td>1.347946</td>\n",
       "      <td>0.754505</td>\n",
       "      <td>1.330642</td>\n",
       "      <td>-0.754453</td>\n",
       "      <td>0.582956</td>\n",
       "      <td>0.252671</td>\n",
       "      <td>1.495870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.113248</td>\n",
       "      <td>-0.607726</td>\n",
       "      <td>-0.947791</td>\n",
       "      <td>0.830851</td>\n",
       "      <td>0.998291</td>\n",
       "      <td>0.498321</td>\n",
       "      <td>-1.493958</td>\n",
       "      <td>0.789572</td>\n",
       "      <td>-1.311018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104698</td>\n",
       "      <td>0.616189</td>\n",
       "      <td>-1.035953</td>\n",
       "      <td>2.111387</td>\n",
       "      <td>-0.984415</td>\n",
       "      <td>1.148076</td>\n",
       "      <td>-1.433554</td>\n",
       "      <td>0.243372</td>\n",
       "      <td>0.170083</td>\n",
       "      <td>1.274795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.223321</td>\n",
       "      <td>-0.479048</td>\n",
       "      <td>-1.925789</td>\n",
       "      <td>1.680377</td>\n",
       "      <td>0.021840</td>\n",
       "      <td>-1.453307</td>\n",
       "      <td>0.605559</td>\n",
       "      <td>-0.019024</td>\n",
       "      <td>1.065448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>-1.957863</td>\n",
       "      <td>-0.123384</td>\n",
       "      <td>1.505329</td>\n",
       "      <td>0.660290</td>\n",
       "      <td>-1.769443</td>\n",
       "      <td>-0.547756</td>\n",
       "      <td>-0.568122</td>\n",
       "      <td>0.244645</td>\n",
       "      <td>0.982116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.160109</td>\n",
       "      <td>0.422684</td>\n",
       "      <td>-0.308029</td>\n",
       "      <td>0.227744</td>\n",
       "      <td>0.432854</td>\n",
       "      <td>0.608348</td>\n",
       "      <td>0.193832</td>\n",
       "      <td>1.035091</td>\n",
       "      <td>-0.538868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416629</td>\n",
       "      <td>1.441766</td>\n",
       "      <td>0.212572</td>\n",
       "      <td>-0.994721</td>\n",
       "      <td>1.143999</td>\n",
       "      <td>-2.166923</td>\n",
       "      <td>-1.199248</td>\n",
       "      <td>-1.028636</td>\n",
       "      <td>0.752791</td>\n",
       "      <td>0.317169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5245</th>\n",
       "      <td>0</td>\n",
       "      <td>1.157565</td>\n",
       "      <td>-0.142219</td>\n",
       "      <td>1.043992</td>\n",
       "      <td>1.144946</td>\n",
       "      <td>1.195423</td>\n",
       "      <td>0.248978</td>\n",
       "      <td>-1.505100</td>\n",
       "      <td>-0.874137</td>\n",
       "      <td>-1.782724</td>\n",
       "      <td>...</td>\n",
       "      <td>1.195423</td>\n",
       "      <td>-0.255793</td>\n",
       "      <td>-0.154838</td>\n",
       "      <td>0.413029</td>\n",
       "      <td>-0.482939</td>\n",
       "      <td>-1.277953</td>\n",
       "      <td>-0.445082</td>\n",
       "      <td>1.195423</td>\n",
       "      <td>-0.924614</td>\n",
       "      <td>-0.432462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5246</th>\n",
       "      <td>0</td>\n",
       "      <td>1.424709</td>\n",
       "      <td>0.235910</td>\n",
       "      <td>1.356778</td>\n",
       "      <td>1.368099</td>\n",
       "      <td>-0.318862</td>\n",
       "      <td>1.039765</td>\n",
       "      <td>-0.986854</td>\n",
       "      <td>-0.330184</td>\n",
       "      <td>-1.383120</td>\n",
       "      <td>...</td>\n",
       "      <td>1.424709</td>\n",
       "      <td>-1.066107</td>\n",
       "      <td>0.881258</td>\n",
       "      <td>-0.488691</td>\n",
       "      <td>-1.281223</td>\n",
       "      <td>-1.213291</td>\n",
       "      <td>0.122692</td>\n",
       "      <td>1.175627</td>\n",
       "      <td>-1.145360</td>\n",
       "      <td>0.451026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5247</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.375687</td>\n",
       "      <td>1.524455</td>\n",
       "      <td>0.012514</td>\n",
       "      <td>-0.007917</td>\n",
       "      <td>0.073809</td>\n",
       "      <td>-0.906909</td>\n",
       "      <td>-1.254247</td>\n",
       "      <td>1.606182</td>\n",
       "      <td>0.298557</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028349</td>\n",
       "      <td>-0.968204</td>\n",
       "      <td>-1.233815</td>\n",
       "      <td>1.626613</td>\n",
       "      <td>-0.191802</td>\n",
       "      <td>1.115823</td>\n",
       "      <td>0.380284</td>\n",
       "      <td>-0.293960</td>\n",
       "      <td>0.135104</td>\n",
       "      <td>1.381434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5248</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.478238</td>\n",
       "      <td>1.666142</td>\n",
       "      <td>0.049609</td>\n",
       "      <td>-0.428752</td>\n",
       "      <td>-0.362771</td>\n",
       "      <td>1.798104</td>\n",
       "      <td>-0.214314</td>\n",
       "      <td>0.775400</td>\n",
       "      <td>-0.379267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.428752</td>\n",
       "      <td>-1.121552</td>\n",
       "      <td>-0.379267</td>\n",
       "      <td>-0.593705</td>\n",
       "      <td>0.049609</td>\n",
       "      <td>1.765114</td>\n",
       "      <td>0.313533</td>\n",
       "      <td>-0.329781</td>\n",
       "      <td>-1.220524</td>\n",
       "      <td>0.033114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5249</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.750874</td>\n",
       "      <td>0.267008</td>\n",
       "      <td>-0.155041</td>\n",
       "      <td>-0.179867</td>\n",
       "      <td>-0.155041</td>\n",
       "      <td>-0.303999</td>\n",
       "      <td>-0.279173</td>\n",
       "      <td>1.731765</td>\n",
       "      <td>0.564925</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.303999</td>\n",
       "      <td>-0.850180</td>\n",
       "      <td>0.937321</td>\n",
       "      <td>-1.594972</td>\n",
       "      <td>1.036626</td>\n",
       "      <td>1.582807</td>\n",
       "      <td>1.036626</td>\n",
       "      <td>-0.254346</td>\n",
       "      <td>0.664230</td>\n",
       "      <td>1.831071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5250 rows × 1201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      labels       f_0       f_1       f_2       f_3       f_4       f_5  \\\n",
       "0          0 -2.033875  0.978446 -0.142131 -0.177117 -1.470684  1.669562   \n",
       "1          1 -0.348835  0.294815 -0.557577 -2.020773 -1.234715  1.633930   \n",
       "2          1  0.113248 -0.607726 -0.947791  0.830851  0.998291  0.498321   \n",
       "3          0  1.223321 -0.479048 -1.925789  1.680377  0.021840 -1.453307   \n",
       "4          0  0.160109  0.422684 -0.308029  0.227744  0.432854  0.608348   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "5245       0  1.157565 -0.142219  1.043992  1.144946  1.195423  0.248978   \n",
       "5246       0  1.424709  0.235910  1.356778  1.368099 -0.318862  1.039765   \n",
       "5247       1 -0.375687  1.524455  0.012514 -0.007917  0.073809 -0.906909   \n",
       "5248       1 -0.478238  1.666142  0.049609 -0.428752 -0.362771  1.798104   \n",
       "5249       1 -0.750874  0.267008 -0.155041 -0.179867 -0.155041 -0.303999   \n",
       "\n",
       "           f_6       f_7       f_8  ...    f_1190    f_1191    f_1192  \\\n",
       "0    -0.196530 -0.125239 -0.452284  ... -1.111266  0.716084  0.060039   \n",
       "1    -1.680658 -0.358146  0.166122  ...  0.735240  0.829781  1.521941   \n",
       "2    -1.493958  0.789572 -1.311018  ...  0.104698  0.616189 -1.035953   \n",
       "3     0.605559 -0.019024  1.065448  ...  0.360237 -1.957863 -0.123384   \n",
       "4     0.193832  1.035091 -0.538868  ...  0.416629  1.441766  0.212572   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5245 -1.505100 -0.874137 -1.782724  ...  1.195423 -0.255793 -0.154838   \n",
       "5246 -0.986854 -0.330184 -1.383120  ...  1.424709 -1.066107  0.881258   \n",
       "5247 -1.254247  1.606182  0.298557  ... -0.028349 -0.968204 -1.233815   \n",
       "5248 -0.214314  0.775400 -0.379267  ... -0.428752 -1.121552 -0.379267   \n",
       "5249 -0.279173  1.731765  0.564925  ... -0.303999 -0.850180  0.937321   \n",
       "\n",
       "        f_1193    f_1194    f_1195    f_1196    f_1197    f_1198    f_1199  \n",
       "0     0.301279 -1.174846 -1.076498 -0.069452 -0.604012 -2.179176  0.558003  \n",
       "1     1.347946  0.754505  1.330642 -0.754453  0.582956  0.252671  1.495870  \n",
       "2     2.111387 -0.984415  1.148076 -1.433554  0.243372  0.170083  1.274795  \n",
       "3     1.505329  0.660290 -1.769443 -0.547756 -0.568122  0.244645  0.982116  \n",
       "4    -0.994721  1.143999 -2.166923 -1.199248 -1.028636  0.752791  0.317169  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5245  0.413029 -0.482939 -1.277953 -0.445082  1.195423 -0.924614 -0.432462  \n",
       "5246 -0.488691 -1.281223 -1.213291  0.122692  1.175627 -1.145360  0.451026  \n",
       "5247  1.626613 -0.191802  1.115823  0.380284 -0.293960  0.135104  1.381434  \n",
       "5248 -0.593705  0.049609  1.765114  0.313533 -0.329781 -1.220524  0.033114  \n",
       "5249 -1.594972  1.036626  1.582807  1.036626 -0.254346  0.664230  1.831071  \n",
       "\n",
       "[5250 rows x 1201 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row has already been flattend and processed hence only to make batches and train a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.iloc[:, 0]\n",
    "features = df.iloc[:, 1:]\n",
    "\n",
    "# Creting tensors by conveting to numpy array and then to tensors.\n",
    "labels = labels.to_numpy()\n",
    "features = features.to_numpy()\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors.\n",
    "labels = torch.from_numpy(labels).long()\n",
    "features = torch.from_numpy(features).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    features, labels, test_size=0.01, random_state=4232)\n",
    "\n",
    "# Using Datasets \n",
    "train_dataset = TensorDataset(features_train, labels_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=160, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels from the train loader dataset\n",
    "class_labels = [label for _, label in train_loader.dataset]\n",
    "\n",
    "# Calculate the class counts\n",
    "class_counts = torch.bincount(torch.tensor(class_labels))\n",
    "total_samples = class_counts.sum()\n",
    "\n",
    "# Convert class counts to real numbers\n",
    "class_counts = class_counts.float()\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "\n",
    "# Create a list of class weights for each training sample\n",
    "sample_weights = [class_weights[label] for _, label in train_loader.dataset]\n",
    "\n",
    "# Create a WeightedRandomSampler with the sample weights\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(train_loader.dataset), replacement=True)\n",
    "\n",
    "# Update the train loader with the weighted sampler\n",
    "train_loader = DataLoader(train_loader.dataset, batch_size=train_loader.batch_size, sampler=sampler, shuffle=False, num_workers=train_loader.num_workers, pin_memory=train_loader.pin_memory)\n",
    "\n",
    "# Rest of your training code using the updated train loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.05)\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512,320)\n",
    "        self.fc4 = nn.Linear(320,128)\n",
    "        self.fc5 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# Set up the model and optimizer\n",
    "input_size = features.shape[1]\n",
    "num_classes = torch.unique(labels).shape[0]\n",
    "model = NeuralNetwork(input_size, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.2099, Accuracy: 0.7736 0.7\n",
      "Epoch 2/300, Loss: 0.1933, Accuracy: 0.8113 0.7368421052631577\n",
      "Epoch 3/300, Loss: 0.1194, Accuracy: 0.8113 0.7222222222222222\n",
      "Epoch 4/300, Loss: 0.0515, Accuracy: 0.9057 0.8571428571428571\n",
      "Epoch 5/300, Loss: 0.0209, Accuracy: 0.8868 0.8125\n",
      "Epoch 6/300, Loss: 0.0043, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 7/300, Loss: 0.0044, Accuracy: 0.7925 0.7027027027027027\n",
      "Epoch 8/300, Loss: 0.0405, Accuracy: 0.7736 0.6666666666666667\n",
      "Epoch 9/300, Loss: 0.0283, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 10/300, Loss: 0.0563, Accuracy: 0.7358 0.6111111111111113\n",
      "Epoch 11/300, Loss: 0.0137, Accuracy: 0.8113 0.75\n",
      "Epoch 12/300, Loss: 0.0123, Accuracy: 0.7358 0.6111111111111113\n",
      "Epoch 13/300, Loss: 0.0374, Accuracy: 0.7547 0.5806451612903226\n",
      "Epoch 14/300, Loss: 0.0236, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 15/300, Loss: 0.0022, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 16/300, Loss: 0.0004, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 17/300, Loss: 0.0058, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 18/300, Loss: 0.0002, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 19/300, Loss: 0.0002, Accuracy: 0.7358 0.5882352941176471\n",
      "Epoch 20/300, Loss: 0.0025, Accuracy: 0.7547 0.6285714285714287\n",
      "Epoch 21/300, Loss: 0.0059, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 22/300, Loss: 0.0028, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 23/300, Loss: 0.0001, Accuracy: 0.8113 0.6875\n",
      "Epoch 24/300, Loss: 0.0076, Accuracy: 0.7925 0.6666666666666667\n",
      "Epoch 25/300, Loss: 0.0215, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 26/300, Loss: 0.0024, Accuracy: 0.8491 0.7647058823529412\n",
      "Epoch 27/300, Loss: 0.0012, Accuracy: 0.8302 0.7428571428571428\n",
      "Epoch 28/300, Loss: 0.0011, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 29/300, Loss: 0.1296, Accuracy: 0.7547 0.6666666666666667\n",
      "Epoch 30/300, Loss: 0.0018, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 31/300, Loss: 0.0015, Accuracy: 0.7925 0.7027027027027027\n",
      "Epoch 32/300, Loss: 0.0217, Accuracy: 0.8113 0.7368421052631577\n",
      "Epoch 33/300, Loss: 0.0391, Accuracy: 0.8491 0.7894736842105262\n",
      "Epoch 34/300, Loss: 0.0393, Accuracy: 0.8491 0.7777777777777778\n",
      "Epoch 35/300, Loss: 0.0028, Accuracy: 0.8491 0.7647058823529412\n",
      "Epoch 36/300, Loss: 0.0002, Accuracy: 0.8491 0.7647058823529412\n",
      "Epoch 37/300, Loss: 0.0009, Accuracy: 0.8491 0.7777777777777778\n",
      "Epoch 38/300, Loss: 0.0003, Accuracy: 0.8868 0.8235294117647058\n",
      "Epoch 39/300, Loss: 0.0009, Accuracy: 0.8302 0.7428571428571428\n",
      "Epoch 40/300, Loss: 0.0042, Accuracy: 0.7736 0.6666666666666667\n",
      "Epoch 41/300, Loss: 0.0081, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 42/300, Loss: 0.0008, Accuracy: 0.7547 0.6486486486486486\n",
      "Epoch 43/300, Loss: 0.0001, Accuracy: 0.7736 0.6666666666666667\n",
      "Epoch 44/300, Loss: 0.0001, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 45/300, Loss: 0.0013, Accuracy: 0.8491 0.7647058823529412\n",
      "Epoch 46/300, Loss: 0.0001, Accuracy: 0.7736 0.6842105263157895\n",
      "Epoch 47/300, Loss: 0.0004, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 48/300, Loss: 0.0010, Accuracy: 0.8491 0.7777777777777778\n",
      "Epoch 49/300, Loss: 0.0228, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 50/300, Loss: 0.0001, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 51/300, Loss: 0.0000, Accuracy: 0.8302 0.7428571428571428\n",
      "Epoch 52/300, Loss: 0.0003, Accuracy: 0.8491 0.7647058823529412\n",
      "Epoch 53/300, Loss: 0.0001, Accuracy: 0.8491 0.7647058823529412\n",
      "Epoch 54/300, Loss: 0.0000, Accuracy: 0.8868 0.8235294117647058\n",
      "Epoch 55/300, Loss: 0.0004, Accuracy: 0.8868 0.8235294117647058\n",
      "Epoch 56/300, Loss: 0.0005, Accuracy: 0.8868 0.8235294117647058\n",
      "Epoch 57/300, Loss: 0.0000, Accuracy: 0.8868 0.8235294117647058\n",
      "Epoch 58/300, Loss: 0.0001, Accuracy: 0.8679 0.7999999999999999\n",
      "Epoch 59/300, Loss: 0.0002, Accuracy: 0.8113 0.7222222222222222\n",
      "Epoch 60/300, Loss: 0.0000, Accuracy: 0.8868 0.8235294117647058\n",
      "Epoch 61/300, Loss: 0.0001, Accuracy: 0.8491 0.7647058823529412\n",
      "Epoch 62/300, Loss: 0.0008, Accuracy: 0.8679 0.787878787878788\n",
      "Epoch 63/300, Loss: 0.0000, Accuracy: 0.8679 0.787878787878788\n",
      "Epoch 64/300, Loss: 0.0000, Accuracy: 0.8679 0.787878787878788\n",
      "Epoch 65/300, Loss: 0.0000, Accuracy: 0.8868 0.8235294117647058\n",
      "Epoch 66/300, Loss: 0.0001, Accuracy: 0.8679 0.787878787878788\n",
      "Epoch 67/300, Loss: 0.0002, Accuracy: 0.8679 0.7741935483870968\n",
      "Epoch 68/300, Loss: 0.0000, Accuracy: 0.7925 0.7027027027027027\n",
      "Epoch 69/300, Loss: 0.0000, Accuracy: 0.8491 0.7777777777777778\n",
      "Epoch 70/300, Loss: 0.0000, Accuracy: 0.8302 0.7567567567567567\n",
      "Epoch 71/300, Loss: 0.0006, Accuracy: 0.8679 0.7999999999999999\n",
      "Epoch 72/300, Loss: 0.0001, Accuracy: 0.8491 0.7777777777777778\n",
      "Epoch 73/300, Loss: 0.0004, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 74/300, Loss: 0.0020, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 75/300, Loss: 0.0004, Accuracy: 0.8491 0.7647058823529412\n",
      "Epoch 76/300, Loss: 0.0001, Accuracy: 0.8679 0.787878787878788\n",
      "Epoch 77/300, Loss: 0.0000, Accuracy: 0.8679 0.787878787878788\n",
      "Epoch 78/300, Loss: 0.0022, Accuracy: 0.8302 0.7567567567567567\n",
      "Epoch 79/300, Loss: 0.0069, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 80/300, Loss: 0.0085, Accuracy: 0.8491 0.7647058823529412\n",
      "Epoch 81/300, Loss: 0.0016, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 82/300, Loss: 0.0016, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 83/300, Loss: 0.0007, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 84/300, Loss: 0.0039, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 85/300, Loss: 0.0000, Accuracy: 0.8679 0.7741935483870968\n",
      "Epoch 86/300, Loss: 0.0010, Accuracy: 0.8491 0.7647058823529412\n",
      "Epoch 87/300, Loss: 0.0005, Accuracy: 0.8491 0.7647058823529412\n",
      "Epoch 88/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 89/300, Loss: 0.0014, Accuracy: 0.8679 0.7741935483870968\n",
      "Epoch 90/300, Loss: 0.0001, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 91/300, Loss: 0.0005, Accuracy: 0.8679 0.7741935483870968\n",
      "Epoch 92/300, Loss: 0.0000, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 93/300, Loss: 0.0003, Accuracy: 0.8679 0.7741935483870968\n",
      "Epoch 94/300, Loss: 0.0000, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 95/300, Loss: 0.0005, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 96/300, Loss: 0.0000, Accuracy: 0.7736 0.6666666666666667\n",
      "Epoch 97/300, Loss: 0.0009, Accuracy: 0.7925 0.6666666666666667\n",
      "Epoch 98/300, Loss: 0.0268, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 99/300, Loss: 0.0036, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 100/300, Loss: 0.0005, Accuracy: 0.7736 0.6666666666666667\n",
      "Epoch 101/300, Loss: 0.0074, Accuracy: 0.7547 0.6486486486486486\n",
      "Epoch 102/300, Loss: 0.0000, Accuracy: 0.7925 0.6451612903225806\n",
      "Epoch 103/300, Loss: 0.0001, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 104/300, Loss: 0.0026, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 105/300, Loss: 0.0003, Accuracy: 0.7925 0.6451612903225806\n",
      "Epoch 106/300, Loss: 0.0001, Accuracy: 0.7547 0.6486486486486486\n",
      "Epoch 107/300, Loss: 0.0000, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 108/300, Loss: 0.0025, Accuracy: 0.7358 0.6111111111111113\n",
      "Epoch 109/300, Loss: 0.0001, Accuracy: 0.7547 0.6285714285714287\n",
      "Epoch 110/300, Loss: 0.0015, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 111/300, Loss: 0.0007, Accuracy: 0.7358 0.6111111111111113\n",
      "Epoch 112/300, Loss: 0.0000, Accuracy: 0.8113 0.7222222222222222\n",
      "Epoch 113/300, Loss: 0.0009, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 114/300, Loss: 0.0000, Accuracy: 0.8302 0.7428571428571428\n",
      "Epoch 115/300, Loss: 0.0000, Accuracy: 0.7925 0.6666666666666667\n",
      "Epoch 116/300, Loss: 0.0002, Accuracy: 0.7736 0.6666666666666667\n",
      "Epoch 117/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 118/300, Loss: 0.0189, Accuracy: 0.8113 0.6875\n",
      "Epoch 119/300, Loss: 0.0000, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 120/300, Loss: 0.0000, Accuracy: 0.7736 0.6666666666666667\n",
      "Epoch 121/300, Loss: 0.0000, Accuracy: 0.7736 0.6666666666666667\n",
      "Epoch 122/300, Loss: 0.0000, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 123/300, Loss: 0.0000, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 124/300, Loss: 0.0000, Accuracy: 0.7547 0.6285714285714287\n",
      "Epoch 125/300, Loss: 0.0000, Accuracy: 0.8113 0.6875\n",
      "Epoch 126/300, Loss: 0.0009, Accuracy: 0.7925 0.6666666666666667\n",
      "Epoch 127/300, Loss: 0.0000, Accuracy: 0.8491 0.7333333333333334\n",
      "Epoch 128/300, Loss: 0.0000, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 129/300, Loss: 0.0000, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 130/300, Loss: 0.0000, Accuracy: 0.8113 0.6875\n",
      "Epoch 131/300, Loss: 0.0000, Accuracy: 0.7925 0.6666666666666667\n",
      "Epoch 132/300, Loss: 0.0002, Accuracy: 0.8113 0.6875\n",
      "Epoch 133/300, Loss: 0.0000, Accuracy: 0.7925 0.6666666666666667\n",
      "Epoch 134/300, Loss: 0.0060, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 135/300, Loss: 0.0000, Accuracy: 0.8113 0.6875\n",
      "Epoch 136/300, Loss: 0.0020, Accuracy: 0.7925 0.6666666666666667\n",
      "Epoch 137/300, Loss: 0.0001, Accuracy: 0.8113 0.6875\n",
      "Epoch 138/300, Loss: 0.0006, Accuracy: 0.7925 0.6666666666666667\n",
      "Epoch 139/300, Loss: 0.0000, Accuracy: 0.8113 0.6875\n",
      "Epoch 140/300, Loss: 0.0001, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 141/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 142/300, Loss: 0.0000, Accuracy: 0.8679 0.787878787878788\n",
      "Epoch 143/300, Loss: 0.0002, Accuracy: 0.8302 0.7428571428571428\n",
      "Epoch 144/300, Loss: 0.0004, Accuracy: 0.8868 0.8125\n",
      "Epoch 145/300, Loss: 0.0001, Accuracy: 0.8868 0.8125\n",
      "Epoch 146/300, Loss: 0.0002, Accuracy: 0.8679 0.7741935483870968\n",
      "Epoch 147/300, Loss: 0.0000, Accuracy: 0.8868 0.8235294117647058\n",
      "Epoch 148/300, Loss: 0.0001, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 149/300, Loss: 0.0000, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 150/300, Loss: 0.0000, Accuracy: 0.8679 0.7586206896551724\n",
      "Epoch 151/300, Loss: 0.0001, Accuracy: 0.8679 0.7741935483870968\n",
      "Epoch 152/300, Loss: 0.0002, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 153/300, Loss: 0.0003, Accuracy: 0.8679 0.7741935483870968\n",
      "Epoch 154/300, Loss: 0.0000, Accuracy: 0.8868 0.8000000000000002\n",
      "Epoch 155/300, Loss: 0.0000, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 156/300, Loss: 0.0000, Accuracy: 0.8491 0.7333333333333334\n",
      "Epoch 157/300, Loss: 0.0000, Accuracy: 0.8679 0.7586206896551724\n",
      "Epoch 158/300, Loss: 0.0000, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 159/300, Loss: 0.0001, Accuracy: 0.8679 0.7586206896551724\n",
      "Epoch 160/300, Loss: 0.0001, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 161/300, Loss: 0.0003, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 162/300, Loss: 0.0000, Accuracy: 0.8302 0.6896551724137931\n",
      "Epoch 163/300, Loss: 0.0000, Accuracy: 0.8679 0.7586206896551724\n",
      "Epoch 164/300, Loss: 0.0001, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 165/300, Loss: 0.0065, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 166/300, Loss: 0.0034, Accuracy: 0.8491 0.7142857142857143\n",
      "Epoch 167/300, Loss: 0.0004, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 168/300, Loss: 0.0000, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 169/300, Loss: 0.0000, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 170/300, Loss: 0.0001, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 171/300, Loss: 0.0001, Accuracy: 0.8113 0.6666666666666667\n",
      "Epoch 172/300, Loss: 0.0001, Accuracy: 0.8679 0.7586206896551724\n",
      "Epoch 173/300, Loss: 0.0000, Accuracy: 0.8679 0.7586206896551724\n",
      "Epoch 174/300, Loss: 0.0000, Accuracy: 0.8679 0.7741935483870968\n",
      "Epoch 175/300, Loss: 0.0001, Accuracy: 0.8491 0.7333333333333334\n",
      "Epoch 176/300, Loss: 0.0000, Accuracy: 0.8679 0.7586206896551724\n",
      "Epoch 177/300, Loss: 0.0000, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 178/300, Loss: 0.0000, Accuracy: 0.8679 0.7586206896551724\n",
      "Epoch 179/300, Loss: 0.0000, Accuracy: 0.8679 0.7586206896551724\n",
      "Epoch 180/300, Loss: 0.0000, Accuracy: 0.8113 0.6875\n",
      "Epoch 181/300, Loss: 0.0000, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 182/300, Loss: 0.0000, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 183/300, Loss: 0.0019, Accuracy: 0.8113 0.6875\n",
      "Epoch 184/300, Loss: 0.0001, Accuracy: 0.7925 0.6451612903225806\n",
      "Epoch 185/300, Loss: 0.0002, Accuracy: 0.8302 0.6896551724137931\n",
      "Epoch 186/300, Loss: 0.0000, Accuracy: 0.8113 0.6875\n",
      "Epoch 187/300, Loss: 0.0000, Accuracy: 0.8113 0.6875\n",
      "Epoch 188/300, Loss: 0.0000, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 189/300, Loss: 0.0128, Accuracy: 0.8491 0.7647058823529412\n",
      "Epoch 190/300, Loss: 0.0000, Accuracy: 0.8491 0.7647058823529412\n",
      "Epoch 191/300, Loss: 0.0001, Accuracy: 0.8113 0.7222222222222222\n",
      "Epoch 192/300, Loss: 0.0000, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 193/300, Loss: 0.0000, Accuracy: 0.8302 0.7428571428571428\n",
      "Epoch 194/300, Loss: 0.0000, Accuracy: 0.8302 0.7428571428571428\n",
      "Epoch 195/300, Loss: 0.0011, Accuracy: 0.8302 0.7428571428571428\n",
      "Epoch 196/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 197/300, Loss: 0.0012, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 198/300, Loss: 0.0213, Accuracy: 0.7925 0.6666666666666667\n",
      "Epoch 199/300, Loss: 0.0000, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 200/300, Loss: 0.0005, Accuracy: 0.8113 0.6875\n",
      "Epoch 201/300, Loss: 0.0001, Accuracy: 0.8113 0.6666666666666667\n",
      "Epoch 202/300, Loss: 0.0007, Accuracy: 0.7736 0.625\n",
      "Epoch 203/300, Loss: 0.0000, Accuracy: 0.7736 0.625\n",
      "Epoch 204/300, Loss: 0.0001, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 205/300, Loss: 0.0004, Accuracy: 0.7925 0.6451612903225806\n",
      "Epoch 206/300, Loss: 0.0004, Accuracy: 0.8302 0.7428571428571428\n",
      "Epoch 207/300, Loss: 0.0000, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 208/300, Loss: 0.0000, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 209/300, Loss: 0.0000, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 210/300, Loss: 0.0000, Accuracy: 0.8113 0.6875\n",
      "Epoch 211/300, Loss: 0.0000, Accuracy: 0.8302 0.7428571428571428\n",
      "Epoch 212/300, Loss: 0.0000, Accuracy: 0.8113 0.6875\n",
      "Epoch 213/300, Loss: 0.0000, Accuracy: 0.8113 0.6875\n",
      "Epoch 214/300, Loss: 0.0000, Accuracy: 0.8491 0.7777777777777778\n",
      "Epoch 215/300, Loss: 0.0015, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 216/300, Loss: 0.0001, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 217/300, Loss: 0.0002, Accuracy: 0.8113 0.6875\n",
      "Epoch 218/300, Loss: 0.0000, Accuracy: 0.7925 0.6666666666666667\n",
      "Epoch 219/300, Loss: 0.0003, Accuracy: 0.8302 0.6896551724137931\n",
      "Epoch 220/300, Loss: 0.0002, Accuracy: 0.8113 0.6666666666666667\n",
      "Epoch 221/300, Loss: 0.0000, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 222/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 223/300, Loss: 0.0001, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 224/300, Loss: 0.0004, Accuracy: 0.7925 0.7027027027027027\n",
      "Epoch 225/300, Loss: 0.0000, Accuracy: 0.8113 0.6875\n",
      "Epoch 226/300, Loss: 0.0007, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 227/300, Loss: 0.0000, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 228/300, Loss: 0.0673, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 229/300, Loss: 0.0000, Accuracy: 0.7925 0.717948717948718\n",
      "Epoch 230/300, Loss: 0.0000, Accuracy: 0.7547 0.6285714285714287\n",
      "Epoch 231/300, Loss: 0.0001, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 232/300, Loss: 0.0000, Accuracy: 0.7736 0.6842105263157895\n",
      "Epoch 233/300, Loss: 0.0000, Accuracy: 0.7736 0.6842105263157895\n",
      "Epoch 234/300, Loss: 0.0000, Accuracy: 0.7736 0.6842105263157895\n",
      "Epoch 235/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 236/300, Loss: 0.0005, Accuracy: 0.7925 0.717948717948718\n",
      "Epoch 237/300, Loss: 0.0001, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 238/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 239/300, Loss: 0.0000, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 240/300, Loss: 0.0000, Accuracy: 0.7736 0.6842105263157895\n",
      "Epoch 241/300, Loss: 0.0021, Accuracy: 0.7925 0.6666666666666667\n",
      "Epoch 242/300, Loss: 0.0006, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 243/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 244/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 245/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 246/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 247/300, Loss: 0.0000, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 248/300, Loss: 0.0001, Accuracy: 0.7736 0.6666666666666667\n",
      "Epoch 249/300, Loss: 0.0001, Accuracy: 0.7736 0.6666666666666667\n",
      "Epoch 250/300, Loss: 0.0000, Accuracy: 0.7925 0.7027027027027027\n",
      "Epoch 251/300, Loss: 0.0284, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 252/300, Loss: 0.0002, Accuracy: 0.8302 0.7096774193548386\n",
      "Epoch 253/300, Loss: 0.0000, Accuracy: 0.7358 0.6315789473684211\n",
      "Epoch 254/300, Loss: 0.0000, Accuracy: 0.7736 0.6470588235294118\n",
      "Epoch 255/300, Loss: 0.0000, Accuracy: 0.7925 0.6666666666666667\n",
      "Epoch 256/300, Loss: 0.0008, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 257/300, Loss: 0.0000, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 258/300, Loss: 0.0000, Accuracy: 0.8113 0.6875\n",
      "Epoch 259/300, Loss: 0.0002, Accuracy: 0.8491 0.7777777777777778\n",
      "Epoch 260/300, Loss: 0.0028, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 261/300, Loss: 0.0004, Accuracy: 0.8491 0.7333333333333334\n",
      "Epoch 262/300, Loss: 0.0000, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 263/300, Loss: 0.0000, Accuracy: 0.8679 0.7741935483870968\n",
      "Epoch 264/300, Loss: 0.0000, Accuracy: 0.8679 0.7741935483870968\n",
      "Epoch 265/300, Loss: 0.0002, Accuracy: 0.8302 0.7567567567567567\n",
      "Epoch 266/300, Loss: 0.0001, Accuracy: 0.8302 0.7567567567567567\n",
      "Epoch 267/300, Loss: 0.0047, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 268/300, Loss: 0.0000, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 269/300, Loss: 0.0040, Accuracy: 0.8113 0.6875\n",
      "Epoch 270/300, Loss: 0.0000, Accuracy: 0.7736 0.6666666666666667\n",
      "Epoch 271/300, Loss: 0.0002, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 272/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 273/300, Loss: 0.0000, Accuracy: 0.7736 0.6666666666666667\n",
      "Epoch 274/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 275/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 276/300, Loss: 0.0002, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 277/300, Loss: 0.0000, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 278/300, Loss: 0.0000, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 279/300, Loss: 0.0000, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 280/300, Loss: 0.0001, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 281/300, Loss: 0.0000, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 282/300, Loss: 0.0002, Accuracy: 0.8491 0.7500000000000001\n",
      "Epoch 283/300, Loss: 0.0000, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 284/300, Loss: 0.0163, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 285/300, Loss: 0.0000, Accuracy: 0.8302 0.7272727272727272\n",
      "Epoch 286/300, Loss: 0.0007, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 287/300, Loss: 0.0015, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 288/300, Loss: 0.0000, Accuracy: 0.7736 0.6666666666666667\n",
      "Epoch 289/300, Loss: 0.0000, Accuracy: 0.8113 0.7058823529411765\n",
      "Epoch 290/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 291/300, Loss: 0.0000, Accuracy: 0.7925 0.6857142857142857\n",
      "Epoch 292/300, Loss: 0.0001, Accuracy: 0.7925 0.7027027027027027\n",
      "Epoch 293/300, Loss: 0.0000, Accuracy: 0.7925 0.7027027027027027\n",
      "Epoch 294/300, Loss: 0.0001, Accuracy: 0.7736 0.6842105263157895\n",
      "Epoch 295/300, Loss: 0.0010, Accuracy: 0.7925 0.7027027027027027\n",
      "Epoch 296/300, Loss: 0.0021, Accuracy: 0.7736 0.625\n",
      "Epoch 297/300, Loss: 0.0019, Accuracy: 0.7925 0.7027027027027027\n",
      "Epoch 298/300, Loss: 0.0000, Accuracy: 0.8113 0.7222222222222222\n",
      "Epoch 299/300, Loss: 0.0000, Accuracy: 0.8113 0.7222222222222222\n",
      "Epoch 300/300, Loss: 0.0000, Accuracy: 0.7358 0.65\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # model = nn.Dropout(p=0.2)\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        f1 = f1_score(labels_test,predicted)\n",
    "        accuracy = (predicted == labels_test).sum().item() / labels_test.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\",f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>...</th>\n",
       "      <th>f_1190</th>\n",
       "      <th>f_1191</th>\n",
       "      <th>f_1192</th>\n",
       "      <th>f_1193</th>\n",
       "      <th>f_1194</th>\n",
       "      <th>f_1195</th>\n",
       "      <th>f_1196</th>\n",
       "      <th>f_1197</th>\n",
       "      <th>f_1198</th>\n",
       "      <th>f_1199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.388242</td>\n",
       "      <td>0.868285</td>\n",
       "      <td>-0.427619</td>\n",
       "      <td>-0.678964</td>\n",
       "      <td>-1.625735</td>\n",
       "      <td>0.262761</td>\n",
       "      <td>1.243040</td>\n",
       "      <td>1.537751</td>\n",
       "      <td>-0.352028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.776403</td>\n",
       "      <td>-0.662884</td>\n",
       "      <td>-0.257091</td>\n",
       "      <td>-1.168413</td>\n",
       "      <td>0.223260</td>\n",
       "      <td>-0.482520</td>\n",
       "      <td>-0.085453</td>\n",
       "      <td>-0.382265</td>\n",
       "      <td>-0.539349</td>\n",
       "      <td>-1.682404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.496920</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.989040</td>\n",
       "      <td>0.451422</td>\n",
       "      <td>0.513516</td>\n",
       "      <td>-0.099658</td>\n",
       "      <td>-1.124326</td>\n",
       "      <td>0.729430</td>\n",
       "      <td>-0.216224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379635</td>\n",
       "      <td>-1.760084</td>\n",
       "      <td>1.125450</td>\n",
       "      <td>-0.328047</td>\n",
       "      <td>-0.880305</td>\n",
       "      <td>-1.257607</td>\n",
       "      <td>0.964312</td>\n",
       "      <td>2.021104</td>\n",
       "      <td>0.655021</td>\n",
       "      <td>-0.423029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.128369</td>\n",
       "      <td>-0.537951</td>\n",
       "      <td>2.544358</td>\n",
       "      <td>1.165254</td>\n",
       "      <td>-1.904994</td>\n",
       "      <td>0.776961</td>\n",
       "      <td>-0.495768</td>\n",
       "      <td>0.060111</td>\n",
       "      <td>-1.418468</td>\n",
       "      <td>...</td>\n",
       "      <td>1.165254</td>\n",
       "      <td>-1.373589</td>\n",
       "      <td>-0.483701</td>\n",
       "      <td>-0.964782</td>\n",
       "      <td>-0.869555</td>\n",
       "      <td>0.066040</td>\n",
       "      <td>-0.444567</td>\n",
       "      <td>-0.531935</td>\n",
       "      <td>-0.878660</td>\n",
       "      <td>1.099488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.051253</td>\n",
       "      <td>1.746814</td>\n",
       "      <td>0.681177</td>\n",
       "      <td>1.844524</td>\n",
       "      <td>-0.327977</td>\n",
       "      <td>1.226839</td>\n",
       "      <td>-0.085519</td>\n",
       "      <td>0.379008</td>\n",
       "      <td>-1.003667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.442288</td>\n",
       "      <td>-2.794472</td>\n",
       "      <td>-0.763468</td>\n",
       "      <td>-0.789832</td>\n",
       "      <td>-0.113209</td>\n",
       "      <td>-2.703150</td>\n",
       "      <td>-2.058728</td>\n",
       "      <td>1.070627</td>\n",
       "      <td>-0.458045</td>\n",
       "      <td>-0.435825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.423209</td>\n",
       "      <td>-0.983594</td>\n",
       "      <td>-1.694170</td>\n",
       "      <td>1.197507</td>\n",
       "      <td>1.044211</td>\n",
       "      <td>0.518777</td>\n",
       "      <td>-0.298612</td>\n",
       "      <td>-0.365174</td>\n",
       "      <td>0.738447</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.624450</td>\n",
       "      <td>-3.200223</td>\n",
       "      <td>0.711422</td>\n",
       "      <td>-0.190394</td>\n",
       "      <td>0.337224</td>\n",
       "      <td>-1.656639</td>\n",
       "      <td>0.707360</td>\n",
       "      <td>-0.562290</td>\n",
       "      <td>1.471181</td>\n",
       "      <td>-0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>2246</td>\n",
       "      <td>0.889888</td>\n",
       "      <td>-0.319077</td>\n",
       "      <td>0.849589</td>\n",
       "      <td>0.822723</td>\n",
       "      <td>0.876455</td>\n",
       "      <td>0.325704</td>\n",
       "      <td>0.876455</td>\n",
       "      <td>-0.910127</td>\n",
       "      <td>0.889888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.889888</td>\n",
       "      <td>-2.226556</td>\n",
       "      <td>-0.090717</td>\n",
       "      <td>-1.393713</td>\n",
       "      <td>-0.896694</td>\n",
       "      <td>-0.399675</td>\n",
       "      <td>-0.856395</td>\n",
       "      <td>0.876455</td>\n",
       "      <td>0.863022</td>\n",
       "      <td>-0.601169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>2247</td>\n",
       "      <td>1.005737</td>\n",
       "      <td>-0.064755</td>\n",
       "      <td>1.163494</td>\n",
       "      <td>1.163494</td>\n",
       "      <td>1.163494</td>\n",
       "      <td>0.724028</td>\n",
       "      <td>0.712760</td>\n",
       "      <td>-0.785929</td>\n",
       "      <td>-1.225394</td>\n",
       "      <td>...</td>\n",
       "      <td>1.163494</td>\n",
       "      <td>-1.270468</td>\n",
       "      <td>-0.932417</td>\n",
       "      <td>-1.169053</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.605636</td>\n",
       "      <td>-0.323927</td>\n",
       "      <td>1.163494</td>\n",
       "      <td>-1.315541</td>\n",
       "      <td>0.047928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>2248</td>\n",
       "      <td>1.252086</td>\n",
       "      <td>1.223561</td>\n",
       "      <td>0.153859</td>\n",
       "      <td>-0.987156</td>\n",
       "      <td>0.239435</td>\n",
       "      <td>-0.003031</td>\n",
       "      <td>-1.158309</td>\n",
       "      <td>1.237823</td>\n",
       "      <td>-1.272410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581740</td>\n",
       "      <td>-1.386512</td>\n",
       "      <td>0.809943</td>\n",
       "      <td>-1.243885</td>\n",
       "      <td>0.153859</td>\n",
       "      <td>-0.630589</td>\n",
       "      <td>1.594391</td>\n",
       "      <td>1.252086</td>\n",
       "      <td>-1.429300</td>\n",
       "      <td>1.408976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>2249</td>\n",
       "      <td>1.042624</td>\n",
       "      <td>-0.129166</td>\n",
       "      <td>1.066538</td>\n",
       "      <td>1.030667</td>\n",
       "      <td>1.162195</td>\n",
       "      <td>0.707827</td>\n",
       "      <td>-1.396612</td>\n",
       "      <td>0.014319</td>\n",
       "      <td>-1.025944</td>\n",
       "      <td>...</td>\n",
       "      <td>1.078495</td>\n",
       "      <td>-1.193343</td>\n",
       "      <td>0.086061</td>\n",
       "      <td>-0.081338</td>\n",
       "      <td>-0.978116</td>\n",
       "      <td>-0.368307</td>\n",
       "      <td>-0.129166</td>\n",
       "      <td>1.090452</td>\n",
       "      <td>-1.444440</td>\n",
       "      <td>0.468686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>2250</td>\n",
       "      <td>-1.319572</td>\n",
       "      <td>-0.485173</td>\n",
       "      <td>-0.098500</td>\n",
       "      <td>2.323293</td>\n",
       "      <td>-0.139202</td>\n",
       "      <td>-0.953250</td>\n",
       "      <td>0.084661</td>\n",
       "      <td>-0.566577</td>\n",
       "      <td>1.427840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505524</td>\n",
       "      <td>-0.220607</td>\n",
       "      <td>-0.871845</td>\n",
       "      <td>0.654495</td>\n",
       "      <td>0.430631</td>\n",
       "      <td>-0.444470</td>\n",
       "      <td>-0.118851</td>\n",
       "      <td>0.471334</td>\n",
       "      <td>-0.078149</td>\n",
       "      <td>-0.566577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2250 rows × 1201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       f_0       f_1       f_2       f_3       f_4       f_5  \\\n",
       "0        1 -3.388242  0.868285 -0.427619 -0.678964 -1.625735  0.262761   \n",
       "1        2 -0.496920  0.952381  0.989040  0.451422  0.513516 -0.099658   \n",
       "2        3  1.128369 -0.537951  2.544358  1.165254 -1.904994  0.776961   \n",
       "3        4  0.051253  1.746814  0.681177  1.844524 -0.327977  1.226839   \n",
       "4        5  1.423209 -0.983594 -1.694170  1.197507  1.044211  0.518777   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "2245  2246  0.889888 -0.319077  0.849589  0.822723  0.876455  0.325704   \n",
       "2246  2247  1.005737 -0.064755  1.163494  1.163494  1.163494  0.724028   \n",
       "2247  2248  1.252086  1.223561  0.153859 -0.987156  0.239435 -0.003031   \n",
       "2248  2249  1.042624 -0.129166  1.066538  1.030667  1.162195  0.707827   \n",
       "2249  2250 -1.319572 -0.485173 -0.098500  2.323293 -0.139202 -0.953250   \n",
       "\n",
       "           f_6       f_7       f_8  ...    f_1190    f_1191    f_1192  \\\n",
       "0     1.243040  1.537751 -0.352028  ... -0.776403 -0.662884 -0.257091   \n",
       "1    -1.124326  0.729430 -0.216224  ...  0.379635 -1.760084  1.125450   \n",
       "2    -0.495768  0.060111 -1.418468  ...  1.165254 -1.373589 -0.483701   \n",
       "3    -0.085519  0.379008 -1.003667  ... -0.442288 -2.794472 -0.763468   \n",
       "4    -0.298612 -0.365174  0.738447  ... -2.624450 -3.200223  0.711422   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2245  0.876455 -0.910127  0.889888  ...  0.889888 -2.226556 -0.090717   \n",
       "2246  0.712760 -0.785929 -1.225394  ...  1.163494 -1.270468 -0.932417   \n",
       "2247 -1.158309  1.237823 -1.272410  ...  0.581740 -1.386512  0.809943   \n",
       "2248 -1.396612  0.014319 -1.025944  ...  1.078495 -1.193343  0.086061   \n",
       "2249  0.084661 -0.566577  1.427840  ... -0.505524 -0.220607 -0.871845   \n",
       "\n",
       "        f_1193    f_1194    f_1195    f_1196    f_1197    f_1198    f_1199  \n",
       "0    -1.168413  0.223260 -0.482520 -0.085453 -0.382265 -0.539349 -1.682404  \n",
       "1    -0.328047 -0.880305 -1.257607  0.964312  2.021104  0.655021 -0.423029  \n",
       "2    -0.964782 -0.869555  0.066040 -0.444567 -0.531935 -0.878660  1.099488  \n",
       "3    -0.789832 -0.113209 -2.703150 -2.058728  1.070627 -0.458045 -0.435825  \n",
       "4    -0.190394  0.337224 -1.656639  0.707360 -0.562290  1.471181 -0.192000  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2245 -1.393713 -0.896694 -0.399675 -0.856395  0.876455  0.863022 -0.601169  \n",
       "2246 -1.169053 -0.008414 -0.605636 -0.323927  1.163494 -1.315541  0.047928  \n",
       "2247 -1.243885  0.153859 -0.630589  1.594391  1.252086 -1.429300  1.408976  \n",
       "2248 -0.081338 -0.978116 -0.368307 -0.129166  1.090452 -1.444440  0.468686  \n",
       "2249  0.654495  0.430631 -0.444470 -0.118851  0.471334 -0.078149 -0.566577  \n",
       "\n",
       "[2250 rows x 1201 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the test data\n",
    "test_features = test_data.iloc[:, 1:].values\n",
    "\n",
    "# labels = labels.to_numpy()\n",
    "# test_features = test_features.to_numpy()\n",
    "\n",
    "# Convert the features to PyTorch tensor\n",
    "test_features = torch.from_numpy(test_features).float()\n",
    "\n",
    "# Create a PyTorch data loader for the test data\n",
    "test_dataset = TensorDataset(test_features)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_features in test_loader:\n",
    "        outputs = model(batch_features[0])\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.tolist())\n",
    "\n",
    "# Create a DataFrame with the index numbers and predictions\n",
    "results = pd.DataFrame({'Index': test_data.index, 'Prediction': predictions})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
